---
chapter: 2
title: Slice Dat Time
shortname: Multitasking
slug: slice-dat-time
updatedAt: 2023-08-02T18:14:02.296Z
---

فرض کنید دارید یک سیستم‌عامل می‌نویسید و می‌خواید کاربرها بتونن چند برنامه رو همزمان اجرا کنن. اما پردازنده‌ی چند هسته‌ای خفنی ندارید، در نتیجه پردازنده‌تون فقط می‌تونه یک دستورالعمل رو در هر لحظه اجرا کنه!

خوشبختانه، شما یک برنامه‌نویس سیستم‌عامل خیلی باهوشین. به این نتیجه می‌رسین که می‌تونید با دادن نوبت به پروسس‌ها روی CPU، ادای موازی‌سازی (parallelism) رو دربیارید. اگه بین پروسس‌ها بچرخید و از هر کدوم چندتا دستورالعمل اجرا کنین، همه‌شون می‌تونن پاسخگو باشن بدون اینکه هیچ فرایند خاصی CPU رو به تنهایی قُرُق کنه.

اما چطور کنترل رو از کد برنامه پس می‌گیرین تا فرایندها رو عوض کنین؟ بعد از کمی تحقیق، متوجه میشین که بیشتر کامپیوترها یک تراشه‌ی تایمر (timer chips) دارن. می‌تونید این تراشه تایمر رو طوری برنامه‌ریزی کنین که بعد از گذشت مدت زمان مشخصی، یک وقفه (interrupt) ایجاد کنه و کنترل رو به یک بخش مخصوص در سیستم‌عامل به نام «کنترل‌کننده وقفه» (interrupt handler) بده.

## وقفه‌های سخت‌افزاری (Hardware Interrupts)

قبل‌تر، از این گفتیم که چطور از وقفه‌های نرم‌افزاری برای انتقال کنترل از یک برنامه‌ی سطح کاربر (userland) به سیستم‌عامل استفاده می‌شه. به این‌ها میگن وقفه‌ی «نرم‌افزاری» چون به صورت داوطلبانه توسط خود برنامه فعال میشن — یعنی کدهای ماشین که پردازنده طبق چرخه‌ی عادی fetch-execute cycle پردازش می‌کنه، بهش دستور میدن که کنترل رو به کرنل منتقل کنه.

<img src='images/keyboard-hardware-interrupt.png' loading='eager' style='max-width: 500px; margin: 0 auto;' alt='A drawing illustrating how hardware interrupts break normal execution. On top: a drawing of a keyboard with a highlighted key, with a lightning bolt drawn to a CPU on the right. On the bottom: some binary labeled "program code," a similar lightning bolt, and some more binary labeled "kernel code." The lightning bolt is labeled "interrupt triggers context switch."' width='935' height='503' />

زمان‌بندهای سیستم‌عامل (OS schedulers) از _تراشه‌های تایمر_ مثل [PITها](https://en.wikipedia.org/wiki/Programmable_interval_timer) (تایمرهای بازه‌ای قابل برنامه‌ریزی) برای ایجاد وقفه‌های سخت‌افزاری جهت انجام مولتی‌تسکینگ استفاده می‌کنن:

1. قبل از پریدن به کد برنامه، سیستم‌عامل، تراشه‌ی تایمر رو طوری تنظیم می‌کنه که بعد از یک مدت زمان مشخص، وقفه ایجاد کنه.
2. سیستم‌عامل به حالت کاربر (user mode) میره و به دستورالعمل بعدی برنامه می‌پره.
3. وقتی زمان تایمر تموم میشه، یک وقفه‌ی سخت‌افزاری ایجاد می‌کنه تا به حالت کرنل (kernel mode) بره و کنترل رو به کد سیستم‌عامل بده.
4. حالا سیستم‌عامل می‌تونه وضعیت فعلی برنامه (جایی که متوقف شده) رو ذخیره کنه، یک برنامه‌ی دیگه رو بارگذاری کنه و این فرایند رو تکرار کنه.

به این میگن _چندوظیفگی پیش‌دستانه (preemptive multitasking)_؛ عمل متوقف کردن یک فرایند رو [_پیش‌دستی (preemption)_](https://en.wikipedia.org/wiki/Preemption_(computing)) میگن. اگه شما، مثلاً، دارید این مقاله رو توی مرورگر می‌خونین و همزمان روی همون دستگاه موسیقی گوش می‌دین، احتمالاً کامپیوتر خودتون داره دقیقاً همین چرخه رو هزاران بار در ثانیه تکرار می‌کنه.

## محاسبه‌ی برش زمانی (Timeslice Calculation)

_برش زمانی (timeslice)_ مدت زمانیه که زمان‌بند (scheduler) سیستم‌عامل به یک فرایند اجازه میده اجرا بشه، قبل از اینکه به طور پیش‌دستانه متوقفش کنه. ساده‌ترین راه برای انتخاب برش‌های زمانی اینه که به همه‌ی فرایندها یک برش زمانی یکسان، مثلاً در محدوده‌ی ۱۰ میلی‌ثانیه، اختصاص بدیم و به ترتیب بین وظایف بچرخیم. به این روش زمان‌بندی _چرخشی نوبتی با برش زمانی ثابت (fixed timeslice round-robin)_ میگن.

> **نکته‌ی باحال در مورد اصطلاحات تخصصی!**
> 
> می‌دونستین که به برش‌های زمانی اغلب «کوانتوم» هم میگن؟ حالا دیگه می‌دونین و می‌تونید دوستای گیکتون رو تحت تأثیر قرار بدین. فکر کنم کلی تشویق لازم دارم که تو این مقاله هر دو جمله یه بار نگفتم کوانتوم.
> 
> حالا که صحبت از اصطلاحات برش زمانی شد، توسعه‌دهندگان کرنل لینوکس از واحد زمانی [جیفی (jiffy)](https://github.com/torvalds/linux/blob/22b8cc3e78f5448b4c5df00303817a9137cd663f/include/linux/jiffies.h) برای شمارش تیک‌های تایمر با فرکانس ثابت استفاده می‌کنن. بین چیزای دیگه، از جیفی‌ها برای اندازه‌گیری طول برش‌های زمانی هم استفاده میشه. فرکانس جیفی در لینوکس معمولاً ۱۰۰۰ هرتزه ولی موقع کامپایل کردن کرنل میشه تغییرش داد.

A slight improvement to fixed timeslice scheduling is to pick a *target latency* — the ideal longest time for a process to respond. The target latency is the time it takes for a process to resume execution after being preempted, assuming a reasonable number of processes. *This is pretty hard to visualize! Don't worry, a diagram is coming soon.*

Timeslices are calculated by dividing the target latency by the total number of tasks; this is better than fixed timeslice scheduling because it eliminates wasteful task switching with fewer processes. With a target latency of 15&nbsp;ms and 10 processes, each process would get 15/10 or 1.5&nbsp;ms to run. With only 3 processes, each process gets a longer 5&nbsp;ms timeslice while still hitting the target latency.

Process switching is computationally expensive because it requires saving the entire state of the current program and restoring a different one. Past a certain point, too small a timeslice can result in performance problems with processes switching too rapidly. It's common to give the timeslice duration a lower bound (*minimum granularity*). This does mean that the target latency is exceeded when there are enough processes for the minimum granularity to take effect.

At the time of writing this article, Linux's scheduler uses a target latency of 6&nbsp;ms and a minimum granularity of 0.75&nbsp;ms.

<img src='images/linux-scheduler-target-latency.png' loading='lazy' style='max-width: 500px; margin: 0 auto;' alt='A diagram titled "Naive Dynamic Timeslice Round-Robin Scheduling." It depicts a time series of 3 different processes getting time to execute in a repeated cycle. In between the execution blocks of each process is a much shorter block labeled "kernel scheduler." The length of each program execution block is labeled "timeslice (2ms)." The distance from the start of process 1 executing to the next start of process 1 executing, encompassing the execution time of processes 2 and 3, is labeled as "target latency (6ms)."' width='935' height='433' />

Round-robin scheduling with this basic timeslice calculation is close to what most computers do nowadays. It's still a bit naive; most operating systems tend to have more complex schedulers which take process priorities and deadlines into account. Since 2007, Linux has used a scheduler called [Completely Fair Scheduler](https://docs.kernel.org/scheduler/sched-design-CFS.html). CFS does a bunch of very fancy computer science things to prioritize tasks and divvy up CPU time.

Every time the OS preempts a process it needs to load the new program's saved execution context, including its memory environment. This is accomplished by telling the CPU to use a different *page table*, the mapping from "virtual" to physical addresses. This is also the system that prevents programs from accessing each other's memory; we'll go down this rabbit hole in chapters [5](/the-translator-in-your-computer) and [6](/lets-talk-about-forks-and-cows) of this article.

## Note #1: Kernel Preemptability

So far, we've been only talking about the preemption and scheduling of userland processes. Kernel code might make programs feel laggy if it took too long handling a syscall or executing driver code.

Modern kernels, including Linux, are [preemptive kernels](https://en.wikipedia.org/wiki/Kernel_preemption). This means they're programmed in a way that allows kernel code itself to be interrupted and scheduled just like userland processes.

This isn't very important to know about unless you're writing a kernel or something, but basically every article I've read has mentioned it so I thought I would too! Extra knowledge is rarely a bad thing.

## Note #2: A History Lesson

Ancient operating systems, including classic Mac OS and versions of Windows long before NT, used a predecessor to preemptive multitasking. Rather than the OS deciding when to preempt programs, the programs themselves would choose to yield to the OS. They would trigger a software interrupt to say, "hey, you can let another program run now." These explicit yields were the only way for the OS to regain control and switch to the next scheduled process.

This is called [*cooperative multitasking*](https://en.wikipedia.org/wiki/Cooperative_multitasking). It has a couple major flaws: malicious or just poorly designed programs can easily freeze the entire operating system, and it's nigh impossible to ensure temporal consistency for realtime/time-sensitive tasks. For these reasons, the tech world switched to preemptive multitasking a long time ago and never looked back.
